import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, Subset
from datetime import timedelta
from sklearn.preprocessing import StandardScaler
import sys, os 
sys.path.append(os.getcwd())
from database_utils import get_engine
from sqlalchemy import text
import joblib

from sklearn.decomposition import PCA
import joblib

##############################################
# Settings
##############################################

# Thá»i gian start vÃ  end (thay Ä‘á»•i náº¿u cáº§n)
START_DATE = "20200102"  # Äá»‹nh dáº¡ng YYYY-MM-DD hoáº·c None náº¿u khÃ´ng giá»›i háº¡n
# START_DATE = "20240627"  # Äá»‹nh dáº¡ng YYYY-MM-DD hoáº·c None náº¿u khÃ´ng giá»›i háº¡n

END_DATE = "20250228"    # Äá»‹nh dáº¡ng YYYY-MM-DD hoáº·c None náº¿u khÃ´ng giá»›i háº¡n

# Constants
STEP_MINUTES = 15  # Khoáº£ng thá»i gian cá»§a má»—i step (phÃºt)
TARGET_LENGTH = (243 // STEP_MINUTES) + 1  # target_0 Ä‘áº¿n target_15
MAX_STEPS = (243 // STEP_MINUTES) + 1      # step_0 Ä‘áº¿n step_15
START_HOUR = 2  # Giáº£ sá»­ dá»¯ liá»‡u báº¯t Ä‘áº§u tá»« 2h má»—i phiÃªn
DELTA = 0.1   # Khoáº£ng chia target thÃ nh cÃ¡c nhÃ£n

SCALE_FACTOR = 1000

# âœ… CÃ¡c Ä‘áº·c trÆ°ng thá»i gian (day_0 Ä‘áº¿n day_6 vÃ  hour_2 Ä‘áº¿n hour_7)
DAY_FEATURES = [f'f{i}' for i in range(6, 13)]
HOUR_FEATURES = [f'f{i}' for i in range(13, 19)]
MONTH_FEATURES = [f'f{i}' for i in range(19, 31)]
LEAP_YEAR_FEATURES = [f'f{i}' for i in range(31, 35)]  # 0 ~ 3
QUARTER_FEATURES = [f'f{i}' for i in range(35, 39)]
HOLIDAYS_FEATURES = ["f176", "f177", "f178", "f179", "f180"]

# âœ… Táº­p há»£p táº¥t cáº£ cÃ¡c time features
TIME_FEATURES = DAY_FEATURES + HOUR_FEATURES + MONTH_FEATURES + LEAP_YEAR_FEATURES + QUARTER_FEATURES + HOLIDAYS_FEATURES

TA_FEATURES = [f'f{i}' for i in range(49, 176)]

ORDERBOOK_INTRADAY_TIMESTEP_FEATURES = [f'f{i}' for i in range(181, 240)] + [f'f{i}' for i in range(319, 349)]

EOD_FEATURES = [f'f{i}' for i in range(240, 319)]

INDEX_FEATURES = []
INDEX_FEATURES = (
        [f'f{i}' for i in range(349, 353)] +
        [f'f{i}' for i in range(353, 361)] +
        [f'f{i}' for i in range(361, 369)] +
        [f'f{i}' for i in range(369, 376)] +
        [f'f{i}' for i in range(376, 396)] +
        [f'f{i}' for i in range(400, 628)] +
        # ["f628"] +
        [f'f{i}' for i in range(629, 753)] +
        ["f757"] 
        # [f'f{i}' for i in range(828, 842)] +
        # [f'f{i}' for i in range(849, 870)] +
        # [f'f{i}' for i in range(877, 891)] +
        # [f'f{i}' for i in range(1018, 1046)] +
        # ['f945'] +
        # [f'f{i}' for i in range(1053, 1258)]
)

SHORT_FEATURES = ['f39', 'f40', 'f41',
                                        'f42', 'f43'] #+ TA_FEATURES
LONG_FEATURES = (['f45', 'f48', 'f47'] +
                 ['f39', 'f40', 'f41'] +
                 ['f42', 'f43'] +
                 TIME_FEATURES
                + TA_FEATURES
                 # + ORDERBOOK_INTRADAY_TIMESTEP_FEATURES
                 # + INDEX_FEATURES

                #  + ["f1102", "f1101", "f1100", "f1099", "f1098", "f1097", "f1096", "f1095", "f1094", "f1093", "f1092", "f1091", "f1090", "f1089", "f1076", "f1075", "f1074", "f1073", "f1072", "f1071", "f1070", "f1066", "f1065", "f1064", "f1063", "f1062", "f1061", "f1060", "f1059", "f1058", "f1057", "f1056", "f1055", "f1054", "f1053", "f945"] #

                #  + ["f1017", "f1016", "f1015", "f1014", "f1013", "f1012", "f1011", "f1010", "f1009", "f419", "f418", "f417", "f416", "f415", "f414", "f413", "f412", "f411", "f410", "f409", "f408", "f407", "f406", "f405", "f404", "f403", "f402", "f401", "f400", "f395", "f394", "f393", "f392", "f391", "f390", "f389", "f388", "f387", "f386", "f385", "f384", "f383", "f382", "f381", "f380", "f379", "f378", "f377", "f376", "f323", "f322", "f321", "f320", "f319", "f185", "f184", "f183", "f182", "f181"] #

                #  + ["f1045", "f1044", "f1043", "f1042", "f1041", "f1040", "f1039", "f1038", "f1037", "f1036", "f1035", "f1034", "f1033", "f1032", "f1031", "f1030", "f1029", "f1028", "f1027", "f1026", "f1025", "f1024", "f1023", "f1022", "f1021", "f1020", "f1019", "f1018", "f890", "f889", "f888", "f887", "f886", "f885", "f884", "f883", "f882", "f881", "f880", "f879", "f878", "f877", "f876", "f875", "f874", "f873", "f872", "f871", "f870", "f869", "f868", "f867", "f866", "f865", "f864", "f863", "f862", "f861", "f860", "f859", "f858", "f857", "f856", "f855", "f854", "f853", "f852", "f851", "f850", "f849", "f848", "f847", "f846", "f845", "f844", "f843", "f842", "f841", "f840", "f839", "f838", "f837", "f836", "f835", "f834", "f833", "f832", "f831", "f830", "f829", "f828", "f827", "f826", "f825", "f824", "f823", "f822", "f821", "f820", "f819", "f818", "f817", "f816", "f815", "f814", "f813", "f812", "f811", "f810", "f809", "f808", "f807", "f806", "f805", "f804", "f803", "f802", "f801", "f800", "f799", "f798", "f797", "f796", "f795", "f794", "f793", "f792", "f791", "f790", "f789", "f788", "f787", "f786", "f785", "f784", "f783", "f782", "f781", "f780", "f779", "f778", "f777", "f776", "f775", "f774", "f773", "f772", "f771", "f770", "f769", "f768", "f767", "f766", "f765", "f764", "f763", "f762", "f761", "f760", "f759", "f758", "f459", "f458", "f457", "f456", "f455", "f454", "f453", "f452", "f451", "f450", "f449", "f448", "f447", "f446", "f445", "f444", "f443", "f442", "f441", "f440", "f439", "f438", "f437", "f436", "f435", "f434", "f433", "f432", "f431", "f430", "f429", "f428", "f427", "f426", "f425", "f424", "f423", "f422", "f421", "f420", "f195", "f194", "f193", "f192", "f191"] #

                #  # + ["f744", "f745", "f753", "f1077", "f1078", "f1079", "f1080", "f1081", "f1082", "f1083", "f1084", "f1085", "f1086", "f1087", "f1088"] #

                #  + ["f935", "f937", "f938", "f939", "f940", "f941", "f946", "f947", "f948", "f949", "f950", "f951", "f952", "f953", "f954", "f955", "f956", "f957", "f1108", "f1109", "f1110", "f1111", "f1112", "f1113", "f1114", "f1115", "f1116", "f1117", "f1118", "f1119", "f1120", "f1121", "f1122", "f1123", "f1124", "f1125", "f1126", "f1127", "f1128", "f1129", "f1130"] #

                #  # + ["f186", "f187", "f188", "f189", "f190", "f396", "f397", "f398", "f399", "f460", "f461", "f462", "f463", "f464", "f465", "f466", "f467", "f468", "f469", "f470", "f471", "f472", "f473", "f474", "f475", "f476", "f477", "f478", "f479", "f480", "f481", "f482", "f483", "f484", "f485", "f486", "f487", "f488", "f489", "f490", "f491", "f492", "f493", "f494", "f495", "f496", "f497", "f498", "f499"] # 
                #  #
                #  # +

                 )

ALL_NEEDED_FEATURES = ["time", "trade_date"] + list(set(SHORT_FEATURES + LONG_FEATURES)) + ["target_0", "target_1", "target_2", "target_3", "target_4", "target_5", "target_6", "target_7", "target_8", "target_9", "target_10", "target_11", "target_12", "target_13", "target_14", "target_15", "target_16"] + [f"target_close_{i}" for i in range(17)] + [f"target_pct_{i}" for i in range(17)]

# LONG_FEATURES += EOD_FEATURES

# print (len(TIME_FEATURES), len(TA_FEATURES))
# exit()


def load_data(load_from_db=False,
              table_name="der_1m_feature",
              eod_table="der_1d_feature",
              der_1m_table="der_1m",
              csv_path="../../data/all_data.csv", save_to_file=True, start_date=START_DATE, end_date=END_DATE):
    """
    Load dá»¯ liá»‡u tá»« database hoáº·c tá»« CSV. Sau Ä‘Ã³ merge vá»›i dá»¯ liá»‡u EOD (chá»‰ sá»‘ káº¿t phiÃªn hÃ´m trÆ°á»›c).
    """
    if load_from_db:
        # Láº¥y engine tá»« database_utils
        engine = get_engine()

        # XÃ¢y dá»±ng danh sÃ¡ch cá»™t cáº§n SELECT cho dá»¯ liá»‡u intraday
        select_columns = ", ".join([f'"{col}"' for col in ALL_NEEDED_FEATURES])

        # Truy váº¥n dá»¯ liá»‡u intraday
        query_str = f'''
            SELECT {select_columns} 
            FROM public."{table_name}"
            WHERE 1=1
            AND "symbol" = 'VN30F1M'
        '''
        if start_date:
            query_str += f' AND "trade_date" >= \'{start_date}\''
        if end_date:
            query_str += f' AND "trade_date" <= \'{end_date}\''

        query_str += ' ORDER BY "time"'
        query = text(query_str)
        df = pd.read_sql(query, con=engine)

        # âœ… Truy váº¥n dá»¯ liá»‡u EOD (chá»‰ láº¥y ngÃ y vÃ  cÃ¡c chá»‰ sá»‘)
        eod_query = f'''
            SELECT "trade_date" AS eod_date, * 
            FROM public."{eod_table}"
            WHERE "trade_date" >= '{start_date}' AND "trade_date" <= '{end_date}'
            ORDER BY "trade_date"
        '''
        eod_df = pd.read_sql(text(eod_query), con=engine)

        # âœ… Dá»‹ch dá»¯ liá»‡u EOD lÃ¹i 1 ngÃ y (vÃ¬ chá»‰ cÃ³ thá»ƒ biáº¿t EOD cá»§a ngÃ y hÃ´m trÆ°á»›c)
        # eod_df["eod_date"] = pd.to_datetime(eod_df["eod_date"])
        # eod_df["merge_date"] = eod_df["eod_date"] + pd.Timedelta(days=1)  # Dá»‹ch ngÃ y lÃªn 1 ngÃ y

        eod_df["merge_date"] = eod_df["eod_date"]  # Dá»‹ch ngÃ y lÃªn 1 ngÃ y
        eod_df.drop(columns=["eod_date"], inplace=True)

        # âœ… Merge vá»›i dá»¯ liá»‡u intraday theo "date"
        # df["trade_date"] = pd.to_datetime(df["trade_date"])
        df = df.merge(eod_df, left_on="trade_date", right_on="merge_date", how="left")
        df.drop(columns=["merge_date"], inplace=True)
        df["trade_date"] = df["trade_date_x"]

        # âœ… Truy váº¥n dá»¯ liá»‡u close_price tá»« der_1m_table
        close_price_query = f'''
            SELECT "time", "symbol", "close" AS close
            FROM public."{der_1m_table}"
            WHERE "trade_date" >= '{start_date}' AND "trade_date" <= '{end_date}'
            AND "symbol" = 'VN30F1M'
        '''
        close_price_df = pd.read_sql(text(close_price_query), con=engine)

        # âœ… Merge vá»›i close_price_df theo "trade_date" vÃ  "time"
        df = df.merge(close_price_df, on=["time"], how="left")

        # âœ… Xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ thiáº¿u (náº¿u cáº§n)
        df["close"].fillna(method="ffill", inplace=True)  # Äiá»n giÃ¡ trá»‹ NaN báº±ng giÃ¡ trÆ°á»›c Ä‘Ã³ náº¿u cÃ³


        # LÆ°u ra CSV náº¿u cáº§n
        if save_to_file:
            df.to_csv(csv_path, index=False)
            print(f"âœ… Dá»¯ liá»‡u Ä‘Ã£ load tá»« DB, merge vá»›i EOD vÃ  lÆ°u ra CSV: {csv_path}")

    else:
        # Äá»c tá»« CSV
        df = pd.read_csv(csv_path)
        print(f"âœ… Dá»¯ liá»‡u Ä‘Ã£ load tá»« CSV: {csv_path}")

    return df


##############################################
# Dataset Class
##############################################
class FinancialDataset(Dataset):
    def __init__(self, data, window_size=60, step_minutes=STEP_MINUTES):
        """
        - Chá»‰ láº¥y dá»¯ liá»‡u táº¡i cÃ¡c má»‘c thá»i gian thá»a mÃ£n `STEP_MINUTES`.
        - Short-term features (chuá»—i dá»¯ liá»‡u trÆ°á»›c thá»i Ä‘iá»ƒm dá»± bÃ¡o).
        - Long-term features láº¥y táº¡i thá»i Ä‘iá»ƒm hiá»‡n táº¡i.
        - Target, mask, percentages, labels láº¥y trá»±c tiáº¿p tá»« dataset.
        """
        self.window_size = window_size
        self.X_short_term = []
        self.X_long_term = []
        self.y = []
        self.masks = []
        self.percentages = []
        self.labels = []
        self.sample_dates = []
        self.sample_times = []
        self.close_prices = []
        self.trade_dates = []

        self.feature_names = LONG_FEATURES
        # self.feature_names = ['f45', 'f104', 'f105', 'f245', 'f1125', 'f368', 'f237', 'f531', 'f1171', 'f1160', 'f42', 'f366', 'f1101', 'f481', 'f1100', 'f355', 'f88', 'f490', 'f239', 'f354', 'f757', 'f47', 'f271', 'f48', 'f1099', 'f303', 'f438', 'f453', 'f250', 'f359', 'f360', 'f1074', 'f1098', 'f487', 'f240', 'f411', 'f582', 'f1126', 'f1155', 'f83', 'f572', 'f253', 'f521', 'f530', 'f341', 'f367', 'f80', 'f1157', 'f68', 'f1073']

        # Chuyá»ƒn toÃ n bá»™ data thÃ nh numpy Ä‘á»ƒ truy xuáº¥t nhanh
        short_term_features = data[SHORT_FEATURES].values
        long_term_features = data[self.feature_names].values
        y_data = data.filter(regex=r"^target_close_").values
        mask_data = data.filter(regex=r"^target_\d+$").values
        percentage_data = data.filter(regex=r"^target_pct_\d+$").values
        label_data = data.filter(regex=r"^target_\d+$").values
        time_data = pd.to_datetime(data["time"])  # Chuyá»ƒn thÃ nh datetime Ä‘á»ƒ xá»­ lÃ½ thá»i gian

        # âœ… Chá»‰ giá»¯ cÃ¡c má»‘c thá»i gian cÃ¡ch nhau STEP_MINUTES tá»« Ä‘áº§u phiÃªn
        start_of_session = time_data.dt.normalize() + pd.Timedelta(hours=START_HOUR)

        # valid_indices = (time_data - start_of_session).dt.total_seconds() % (step_minutes * 60) == 0
        valid_indices = np.ones(len(time_data), dtype=bool)  # Giá»¯ táº¥t cáº£ cÃ¡c má»‘c thá»i gian

        filtered_indices = np.where(valid_indices)[0]  # Láº¥y index cá»§a cÃ¡c Ä‘iá»ƒm cáº§n dá»± bÃ¡o

        for i in filtered_indices:
            # âœ… Láº¥y cá»­a sá»• dá»¯ liá»‡u `window_size` bÆ°á»›c vá» trÆ°á»›c cho short-term features
            start_idx = max(0, i - window_size + 1)
            short_term_window = short_term_features[start_idx:i + 1]

            # Náº¿u thiáº¿u dá»¯ liá»‡u á»Ÿ Ä‘áº§u, pad báº±ng 0
            if len(short_term_window) < window_size:
                pad_length = window_size - len(short_term_window)
                short_term_window = np.pad(short_term_window, ((pad_length, 0), (0, 0)), mode='constant')

            # LÆ°u láº¡i cÃ¡c giÃ¡ trá»‹ vÃ o danh sÃ¡ch
            self.X_short_term.append(short_term_window)
            self.X_long_term.append(long_term_features[i])
            self.y.append(y_data[i])
            self.masks.append(mask_data[i])
            self.percentages.append(percentage_data[i])

            # self.labels.append(label_data[i])
            self.labels.append(np.where(label_data[i] < 0, -1, np.where(label_data[i] > 0, 1, 0)))

            self.sample_dates.append(pd.to_datetime(data["trade_date_x"].iloc[i], format='%Y%m%d', errors='coerce'))
            self.trade_dates.append(data["trade_date_x"].iloc[i])
            self.sample_times.append(data["time"].iloc[i])
            self.close_prices.append(data["close"].iloc[i])


        # Chuyá»ƒn thÃ nh numpy arrays Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™ truy xuáº¥t
        self.X_short_term = np.array(self.X_short_term)
        self.X_long_term = np.array(self.X_long_term)
        self.y = np.array(self.y)
        self.masks = np.array(self.masks)
        self.percentages = np.array(self.percentages)
        self.labels = np.array(self.labels)
        self.close_prices = np.array(self.close_prices)

    def scale_data(self, scaler):
        # Scale short-term price features
        num_price_features = 5
        price_data = self.X_short_term[:, :, :num_price_features]
        price_data_scaled = scaler['short_term'].transform(price_data.reshape(-1, num_price_features)).reshape(price_data.shape)

        self.X_short_term = np.concatenate([price_data_scaled, self.X_short_term[:, :, num_price_features:]], axis=2)

        # Scale long-term price features
        num_long_term_price_features = 3
        long_term_price_data = self.X_long_term[:, :num_long_term_price_features]
        long_term_price_data_scaled = scaler['long_term'].transform(long_term_price_data)

        self.X_long_term = np.concatenate([long_term_price_data_scaled, self.X_long_term[:, num_long_term_price_features:]], axis=1)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return (torch.tensor(self.X_short_term[idx], dtype=torch.float32),
                torch.tensor(self.X_long_term[idx], dtype=torch.float32),
                torch.tensor(self.y[idx], dtype=torch.float32),
                torch.tensor(self.masks[idx], dtype=torch.float32),
                torch.tensor(self.percentages[idx], dtype=torch.float32),
                torch.tensor(self.labels[idx], dtype=torch.long),
                torch.tensor(self.close_prices[idx], dtype=torch.long)
                )


##############################################
# Scaling
##############################################
def compute_scalers(train_dataset):
    short_term_scaler = StandardScaler()
    long_term_scaler = StandardScaler()

    num_price_features = 5
    short_term_data = train_dataset.dataset.X_short_term[:, :, :num_price_features].reshape(-1, num_price_features)
    long_term_data = train_dataset.dataset.X_long_term[:, :3]

    short_term_scaler.fit(short_term_data)
    long_term_scaler.fit(long_term_data)

    return {'short_term': short_term_scaler, 'long_term': long_term_scaler}

##############################################
# Load Train/Val/Test
##############################################
def get_train_val_test():
    # data = pd.read_csv('D:\Quant_Prj_Goline\processed_data\OHLC_VN30F1M_processed.csv')
    # data = pd.read_csv('../../data/all_data.csv')
    # data = pd.read_csv('../../data/all_data.csv', nrows=1000)

    data = load_data(load_from_db=False, csv_path="data/all_data.csv")
    # data = load_data(load_from_db=False)

    # Fill NaN values with 0
    data.fillna(0, inplace=True)

    dataset = FinancialDataset(data, window_size=30)

    all_dates = sorted(set(pd.to_datetime(dataset.sample_dates, errors='coerce').dropna()))

    last_date = pd.to_datetime(all_dates[-1])
    first_date = pd.to_datetime(all_dates[0])

    test_length = timedelta(days=30)
    test_start_date = last_date - test_length + timedelta(days=1)

    val_length = timedelta(days=30)
    val_start_date = test_start_date - val_length

    train_length = timedelta(days=18000)
    train_start_date = val_start_date - train_length

    train_dates = [date for date in all_dates if train_start_date <= pd.to_datetime(date) < val_start_date]
    val_dates = [date for date in all_dates if val_start_date <= pd.to_datetime(date) < test_start_date]
    test_dates = [date for date in all_dates if pd.to_datetime(date) >= test_start_date]

    train_indices = [i for i, d in enumerate(dataset.sample_dates) if d in train_dates]
    val_indices = [i for i, d in enumerate(dataset.sample_dates) if d in val_dates]
    test_indices = [i for i, d in enumerate(dataset.sample_dates) if d in test_dates]

    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)

    # scaler = compute_scalers(train_dataset)
    #
    # # Save scaler to scaler.pkl
    # # Save scalers to a file
    # joblib.dump(scaler, "scaler.pkl")
    # print("âœ… Scalers saved to scaler.pkl")
    #
    # dataset.scale_data(scaler)

    train_dataset = Subset(dataset, train_indices)
    val_dataset = Subset(dataset, val_indices)
    test_dataset = Subset(dataset, test_indices)

    print(f"Train: {train_dates[0]} to {train_dates[-1]} ({len(train_dates)} days)")
    print(f"Validation: {val_dates[0]} to {val_dates[-1]} ({len(val_dates)} days)") 
    print(f"Test: {test_dates[0]} to {test_dates[-1]} ({len(test_dates)} days)")

    return dataset, train_dataset, val_dataset, test_dataset, train_indices, val_indices, test_indices


def apply_pca_auto(train_dataset, val_dataset, test_dataset, variance_threshold=0.95, save_path="pca.pkl"):
    """
    Tá»± Ä‘á»™ng chá»n sá»‘ chiá»u PCA Ä‘á»ƒ giá»¯ Ã­t nháº¥t `variance_threshold` phÆ°Æ¡ng sai cá»§a dá»¯ liá»‡u train.

    Parameters:
        - train_dataset (Subset): Dá»¯ liá»‡u training.
        - val_dataset (Subset): Dá»¯ liá»‡u validation.
        - test_dataset (Subset): Dá»¯ liá»‡u test.
        - variance_threshold (float): Tá»· lá»‡ phÆ°Æ¡ng sai muá»‘n giá»¯ láº¡i (0 < variance_threshold <= 1).
        - save_path (str): ÄÆ°á»ng dáº«n lÆ°u PCA model Ä‘Ã£ Ä‘Æ°á»£c fit.

    Returns:
        - PCA-fitted datasets (train, val, test) dÆ°á»›i dáº¡ng numpy arrays.
        - Sá»‘ lÆ°á»£ng thÃ nh pháº§n chÃ­nh Ä‘Ã£ chá»n.
    """

    # Láº¥y dá»¯ liá»‡u tá»« Subset Dataset
    X_train = np.array([train_dataset.dataset.X_long_term[i] for i in train_dataset.indices])
    X_val = np.array([val_dataset.dataset.X_long_term[i] for i in val_dataset.indices])
    X_test = np.array([test_dataset.dataset.X_long_term[i] for i in test_dataset.indices])

    # âœ… Fit PCA vá»›i sá»‘ chiá»u khÃ´ng giá»›i háº¡n, Ä‘á»ƒ kiá»ƒm tra bao nhiÃªu cáº§n giá»¯ láº¡i `variance_threshold`
    pca_full = PCA().fit(X_train)

    # âœ… TÃ­nh toÃ¡n sá»‘ chiá»u cáº§n thiáº¿t Ä‘á»ƒ giá»¯ láº¡i `variance_threshold` phÆ°Æ¡ng sai
    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)
    n_components = np.argmax(cumulative_variance >= variance_threshold) + 1

    print(f"ğŸ”¹ PCA chá»n {n_components} thÃ nh pháº§n chÃ­nh Ä‘á»ƒ giá»¯ láº¡i {variance_threshold * 100:.1f}% phÆ°Æ¡ng sai")

    # âœ… Fit láº¡i PCA vá»›i sá»‘ chiá»u Ä‘Ã£ chá»n
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train)

    # âœ… Ãp dá»¥ng PCA Ä‘Ã£ fit lÃªn táº­p val & test
    X_val_pca = pca.transform(X_val)
    X_test_pca = pca.transform(X_test)

    # âœ… LÆ°u PCA Ä‘Ã£ Ä‘Æ°á»£c fit Ä‘á»ƒ dÃ¹ng láº¡i
    joblib.dump(pca, save_path)
    print(f"âœ… PCA model saved to {save_path}")

    return X_train_pca, X_val_pca, X_test_pca, n_components


if __name__ == "__main__":
    _, train_dataset, val_dataset, test_dataset, _, _, _ = get_train_val_test()
    print("âœ… Data loaded successfully!")

    # âœ… Cháº¡y PCA tá»± Ä‘á»™ng chá»n sá»‘ chiá»u Ä‘á»ƒ giá»¯ Ã­t nháº¥t 95% phÆ°Æ¡ng sai
    X_train_pca, X_val_pca, X_test_pca, num_components = apply_pca_auto(train_dataset, val_dataset, test_dataset, variance_threshold=0.95)

    print(f"Sá»‘ chiá»u Ä‘Æ°á»£c chá»n: {num_components}")

