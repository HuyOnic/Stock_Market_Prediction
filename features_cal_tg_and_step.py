import psycopg2
import pandas as pd
import numpy as np
import logging
from database_utils import get_engine
from sqlalchemy import text
from multiprocessing import Pool, cpu_count
from features_get_encoded_feature_name import get_or_create_feature_name

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
STEP_MINUTES = 15  # Kho·∫£ng th·ªùi gian c·ªßa m·ªói step (ph√∫t)
TARGET_LENGTH = (243 // STEP_MINUTES) + 1  # target_0 ƒë·∫øn target_15
MAX_STEPS = (243 // STEP_MINUTES) + 1      # step_0 ƒë·∫øn step_15
START_HOUR = 2  # Gi·∫£ s·ª≠ d·ªØ li·ªáu b·∫Øt ƒë·∫ßu t·ª´ 2h m·ªói phi√™n
DELTA = 0.1   # Kho·∫£ng chia target th√†nh c√°c nh√£n

# H√†m chuy·ªÉn ƒë·ªïi ph·∫ßn trƒÉm thay ƒë·ªïi th√†nh nh√£n, √©p ki·ªÉu v·ªÅ int c·ªßa Python
def percent_to_label(percent, delta=DELTA):
    percent = np.clip(percent, -10.0, 10.0) # Convert -10 and 10 to float
    return int(round(percent / delta))

def label_to_percent(label, delta=DELTA):
    return label * delta

# H√†m t·∫°o c√°c c·ªôt n·∫øu ch∆∞a t·ªìn t·∫°i trong b·∫£ng
def create_missing_columns():
    engine = get_engine()
    with engine.connect() as conn:
        alter_commands = []
        # C√°c c·ªôt step l√† ki·ªÉu s·ªë th·ª±c (float)
        for j in range(MAX_STEPS + 1):
            feature_name = get_or_create_feature_name(f'step_{j}')
            alter_commands.append(f'ADD COLUMN IF NOT EXISTS "{feature_name}" float')  # Add j to colname
        # C√°c c·ªôt target l√† ki·ªÉu s·ªë nguy√™n (integer)
        for j in range(TARGET_LENGTH):
            alter_commands.append(f'ADD COLUMN IF NOT EXISTS "target_{j}" integer')  # Add j to colname
        alter_query = 'ALTER TABLE public."der_1m_feature" ' + ", ".join(alter_commands) + ";"
        logger.info("‚úÖ T·∫°o c·ªôt n·∫øu ch∆∞a t·ªìn t·∫°i...")
        conn.execute(text(alter_query))
        conn.commit()


def compute_future_values(group):
    group = group.sort_values('time')  # ƒê·∫£m b·∫£o d·ªØ li·ªáu theo th·ª© t·ª± th·ªùi gian

    # T·∫°o c·ªôt 'time_prev' v·ªõi gi√° tr·ªã 15 ph√∫t tr∆∞·ªõc ƒë√≥
    group['time_prev'] = group['time'] - pd.Timedelta(minutes=STEP_MINUTES)

    # X√°c ƒë·ªãnh kho·∫£ng ngh·ªâ tr∆∞a (gi·∫£ s·ª≠ ngh·ªâ t·ª´ 04:30:00 ƒë·∫øn 06:00:00)
    mask_lunch_break = (group['time_prev'].dt.time >= pd.to_datetime("04:30:00").time()) & \
                       (group['time_prev'].dt.time < pd.to_datetime("06:00:00").time())

    # üî• Fix: ƒê·ªëi v·ªõi d·ªØ li·ªáu ƒë·∫ßu ti√™n sau ngh·ªâ tr∆∞a (>= 06:00:00)
    # T√¨m gi√° tr·ªã g·∫ßn nh·∫•t tr∆∞·ªõc ƒë√≥ (trong phi√™n s√°ng) ƒë·ªÉ tham chi·∫øu
    first_after_lunch = group.loc[group['time'].dt.time >= pd.to_datetime("06:00:00").time()].index.min()
    last_before_lunch = group.loc[group['time'].dt.time < pd.to_datetime("04:30:00").time()].index.max()

    if pd.notna(first_after_lunch) and pd.notna(last_before_lunch):
        group.at[first_after_lunch, 'time_prev'] = group.at[last_before_lunch, 'time']

    # Lo·∫°i b·ªè NaN trong time_prev tr∆∞·ªõc khi merge_asof
    group = group.dropna(subset=['time_prev'])

    # **üî• Fix: S·∫Øp x·∫øp l·∫°i `group` theo `time_prev` tr∆∞·ªõc khi merge**
    group = group.sort_values('time_prev')

    # N·∫øu nh√≥m r·ªóng sau khi lo·∫°i b·ªè, tr·∫£ v·ªÅ to√†n 0
    if group.empty:
        return np.zeros(len(group))

    # D√πng merge_asof ƒë·ªÉ t√¨m gi√° tr·ªã close c·ªßa 15 ph√∫t tr∆∞·ªõc
    group = pd.merge_asof(
        group,
        group[['time', 'close']].rename(columns={'time': 'time_prev', 'close': 'close_prev'}),
        on='time_prev',
        direction='backward'
    )

    # T√≠nh ph·∫ßn trƒÉm thay ƒë·ªïi
    group['future_change'] = ((group['close'] - group['close_prev']) / group['close_prev']) * 100

    # ƒêi·ªÅn 0 n·∫øu kh√¥ng t√¨m th·∫•y gi√° tr·ªã h·ª£p l·ªá
    group['future_change'] = group['future_change'].fillna(0)

    return group['future_change'].to_numpy()


def compute_labels_for_group(group_values, batch, date):
    labels = []
    date_batch = batch[batch['trade_date'] == date]
    for group_idx in date_batch['group_index']:
        start_idx = group_idx + 1
        end_idx = start_idx + TARGET_LENGTH
        future_changes = [percent_to_label(p) for p in group_values[start_idx:end_idx]]
        padded_changes = np.pad(
            future_changes,
            (0, max(0, TARGET_LENGTH - len(future_changes))),
            mode='constant'
        )
        labels.append(padded_changes)
    return np.vstack(labels) if labels else np.array([])


def process_batch(batch):
    engine = get_engine()
    with engine.connect() as conn:
        batch = batch.reset_index(drop=True)
        batch = batch[batch['timestamp'] % STEP_MINUTES == 0].reset_index(drop=True)

        # T·∫°o index theo b∆∞·ªõc 15 ph√∫t

        # batch['group_index'] = (batch['timestamp'] // STEP_MINUTES).astype(int)
        batch['group_index'] = batch.groupby('trade_date').cumcount()

        batch['step_index'] = np.clip(batch['group_index'], 0, MAX_STEPS)
        step_one_hot_matrix = np.eye(MAX_STEPS + 1)[batch['step_index']]

        # √Åp d·ª•ng groupby theo 'date'
        future_values_series = batch.groupby('trade_date', group_keys=False).apply(compute_future_values)

        future_values = {date: future_values_series[date] for date in batch['trade_date'].unique()}  # Removed .to_numpy()

        # print (batch)
        # print (future_values_series)

        # T·∫°o nh√£n ch√≠nh x√°c cho t·ª´ng d√≤ng trong batch
        future_labels = {}
        for date in batch['trade_date'].unique():
            group_values = future_values.get(date)
            if group_values is not None:
                labels = compute_labels_for_group(group_values, batch, date)
                future_labels[date] = labels
            else:
                future_labels[date] = np.array([])

        # print (future_labels)

        batch_update_data = []
        for i in range(len(batch)):
            result_row = {'time': batch['time'].iloc[i].to_pydatetime()}
            for j in range(MAX_STEPS + 1):
                feature_name = get_or_create_feature_name(f'step_{j}')
                result_row[feature_name] = float(step_one_hot_matrix[i, j])

            group_step_index = batch['group_index'].iloc[i]
            current_date = batch['trade_date'].iloc[i]

            matrix = future_labels.get(current_date)
            if matrix is not None and matrix.size > 0 and 0 <= group_step_index < matrix.shape[0]:
                for j in range(TARGET_LENGTH):
                    result_row[f'target_{j}'] = int(matrix[group_step_index, j])
            else:
                for j in range(TARGET_LENGTH):
                    result_row[f'target_{j}'] = 0

            batch_update_data.append(result_row)

        if batch_update_data:
            # Chu·∫©n b·ªã danh s√°ch c√°c c·ªôt
            step_columns = [f'"{get_or_create_feature_name(f"step_{j}")}"' for j in range(MAX_STEPS + 1)]
            target_columns = [f'"target_{i}"' for i in range(TARGET_LENGTH)]
            all_columns = ['"time"'] + step_columns + target_columns

            # T·∫°o danh s√°ch gi√° tr·ªã
            values_str = ", ".join([
                f"('{row['time']}'::timestamp, " +
                ", ".join([str(row[feature]) for feature in step_columns]) + ", " +
                ", ".join([str(row[f'target_{i}']) for i in range(TARGET_LENGTH)]) +
                ")"
                for row in batch_update_data
            ])

            # T·∫°o truy v·∫•n c·∫≠p nh·∫≠t h√†ng lo·∫°t
            update_query = f"""
            INSERT INTO public."der_1m_feature" ({", ".join(all_columns)})
            VALUES {values_str}
            ON CONFLICT ("time") DO UPDATE SET
            {", ".join([f"{col} = EXCLUDED.{col}" for col in step_columns + target_columns])};
            """

            # Th·ª±c thi truy v·∫•n c·∫≠p nh·∫≠t h√†ng lo·∫°t
            conn.execute(text(update_query))
            conn.commit()



if __name__ == "__main__":
    try:
        engine = get_engine()
        with engine.connect() as conn:
            logger.info("‚úÖ K·∫øt n·ªëi PostgreSQL th√†nh c√¥ng!")

            # T·∫°o c√°c c·ªôt c·∫ßn thi·∫øt n·∫øu ch∆∞a t·ªìn t·∫°i
            create_missing_columns()

            # --- X√ìA H·∫æT C√ÅC GI√Å TR·ªä HI·ªÜN T·∫†I TRONG C√ÅC C·ªòT computed (step, target)
            # cho nh·ªØng d√≤ng m√† ph√∫t c·ªßa "time" kh√¥ng ph·∫£i l√† b·ªôi s·ªë c·ªßa 15
            clear_query = text(
                """
                UPDATE public."der_1m_feature"
                SET """ + ", ".join([f'"{get_or_create_feature_name(f"step_{j}")}" = NULL' for j in range(MAX_STEPS + 1)] + [f'"target_{i}" = NULL' for i in range(TARGET_LENGTH)]) + """
                WHERE EXTRACT(MINUTE FROM "time")::int % 15 <> 0;
                """
            )
        logger.info("‚úÖ ƒêang x√≥a c√°c gi√° tr·ªã computed ·ªü nh·ªØng d√≤ng kh√¥ng c√≥ ph√∫t ch·∫µn 15...")
        conn.execute(clear_query)
        conn.commit()
        # --- K·∫æT TH√öC PH·∫¶N X√ìA

        # ƒê·ªçc d·ªØ li·ªáu c·∫ßn thi·∫øt (bao g·ªìm c√°c d√≤ng c√≥ ph√∫t ch·∫µn v√† kh√¥ng ch·∫µn)
        query = (
            'SELECT f1."time", f1."trade_date", ohlc."close" '
            'FROM public."der_1m_feature" f1 '
            'JOIN public."der_1m" ohlc ON f1."time" = ohlc."time" '
            'ORDER BY f1."time";'
        )
        df = pd.read_sql(query, conn)
        logger.info("‚úÖ ƒê·ªçc d·ªØ li·ªáu th√†nh c√¥ng!")

        df['time'] = pd.to_datetime(df['time'])
        # T√≠nh timestamp theo s·ªë ph√∫t th·∫≠t t·ª´ gi·ªù b·∫Øt ƒë·∫ßu, kh√¥ng l√†m tr√≤n v·ªÅ b·ªôi s·ªë c·ªßa 15
        df['timestamp'] = ((df['time'].dt.hour - START_HOUR) * 60 + df['time'].dt.minute)

        # T·∫°o batch theo ng√†y ƒë·ªÉ x·ª≠ l√Ω song song
        unique_dates = df['trade_date'].unique()
        # unique_dates = df['date'].unique()[:1]

        batch_size = max(1, len(unique_dates) // cpu_count())
        # batch_size = 1

        batches = [df[df['trade_date'].isin(batch)].copy() for batch in np.array_split(unique_dates, batch_size)]



        with Pool(cpu_count()) as pool:
            pool.map(process_batch, batches)

        # for batch in batches:
        #     process_batch(batch)

        logger.info("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√†nh c√¥ng!")

    except Exception as e:
        logger.error(f"‚ùå L·ªói: {str(e)}")
        raise (e)

    finally:
        logger.info("ƒê√£ ho√†n t·∫•t x·ª≠ l√Ω.")